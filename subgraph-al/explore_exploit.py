from scipy.spatial.distance import cdist
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from collections import Counter
from datetime import datetime
import pyprind
import sys
import time


class DistanceCalculator:
    # def __init__(self, mx, typ="euclidean"):
    #     self._mx = mx
    #     self._type = typ

    def euclidean(self, mx1, mx2, typ="euclidean"):
        # Get euclidean distances as 2D array
        dists = cdist(mx1, mx2, typ)
        # return the most distant rows
        # mx1 - test matrix
        # mx2 - train matrix
        # index of max value in dists (r, c)
        # TODO change to average distance ?
        # TODO RETURN K
        # TODO Think about first reveal
        return np.unravel_index(dists.argmax(), (mx1.shape[0], mx2.shape[0]))

    # TODO:implement using one class svm- http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
    def one_class_learning(self, x_train, x_test):
        model = svm.OneClassSVM()
        model.fit(x_train)
        pred = model.predict(x_test)
        # -1 is abnormal group
        indices = np.where(pred == -1)[0]
        if len(indices) >= 1:
            random_idx = np.random.randint(0, len(indices))
            # TODO RETURN K
            return indices[random_idx]
        else:
            return np.random.randint(0, len(x_test))


class Learning:
    def machine_learning(self, x_train, y_train, x_test, smallest_class, clf=None):
        # smallest class -> 1/0
        if clf is None:
            # n_estimators - number of trees
            # balances -
            clf = RandomForestClassifier(n_estimators=200, class_weight="balanced")
        clf.fit(np.asmatrix(x_train, dtype=np.float32), y_train)
        probs = clf.predict_proba(x_test)
        # probs closer to 0 -> 0
        return probs.argmax(0)[smallest_class]  # smallest class black

    # TODO: implementation using keras
    def deep_learning(self, x_train, y_train, x_test, smallest_class):
        # pass
        from keras import Sequential
        from keras.callbacks import EarlyStopping
        from keras.layers import Dense, Dropout
        from keras.regularizers import l1_l2
        # stop if there is no improvement
        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min', verbose=1)
        self.classifier = Sequential()
        # he_normal - init weights normal
        self.classifier.add(Dense(300, kernel_initializer="he_normal", activation="relu", input_dim=x_train.shape[1]))
        self.classifier.add(Dropout(0.5))
        self.classifier.add(Dense(100, kernel_initializer='he_normal', activation='relu', kernel_regularizer=l1_l2(0.5)))
        self.classifier.add(Dropout(0.5))
        self.classifier.add(Dense(20, kernel_initializer='he_normal', activation='relu', kernel_regularizer=l1_l2(0.5)))
        self.classifier.add(Dropout(0.5))
        self.classifier.add(Dense(1, kernel_initializer='uniform', activation="sigmoid", kernel_regularizer=l1_l2(0.1)))

        self.classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        self.classifier.fit(x_train, y_train, validation_split=0.1, callbacks=[early_stopping], epochs=10,
                            batch_size=520, verbose=0)
        probs = self.classifier.predict_proba(x_test)
        return probs.argmax()


class ExploreExploit:
    def __init__(self, tags_vect, features_mx, recall, eps):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1][0]  # who is the black
        self._n_black = Counter(tags_vect).most_common()[-1][1]  # number of blacks
        self._stop_cond = np.round(recall * self._n_black)  # number of blacks to find - stop condition
        self._eps = eps
        self._time = 0          # how many nodes we asked about
        self._num_black_found = 0
        self._num_nodes = len(tags_vect)    # number of all nodes

    def _init(self):
        # initialize the train objects with copies - COPY
        # self.x_test = type(self._features_mx)(self._features_mx)
        self.x_test = self._features_mx.copy()
        self.y_test = list(self._tags)
        self.x_train = None
        self.y_train = None
        self.bar = pyprind.ProgBar(len(self._tags), stream=sys.stdout)  # optional
        # explore first using distance
        idx1, idx2 = DistanceCalculator().euclidean(self.x_test, self.x_test)  # first two nodes (index)
        self._reveal(idx1)
        self._reveal(idx2)

    def run(self, dist_calc_type):
        self._num_black_found = 0
        self._init()
        # while not 0.7 recall
        while self._num_black_found < self._stop_cond:
            rand = np.random.uniform(0, 1)
            # 0 < a < eps -> distance based -> one_class/ euclidean  || at least one black and white reviled
            if rand < self._eps or not len(Counter(self.y_train)) > 1:
                # idx -> most far away node index
                if dist_calc_type == "euclidean":
                    idx = DistanceCalculator().euclidean(self.x_test, self.x_train)[0]
                else:
                    idx = DistanceCalculator().one_class_learning(self.x_train, self.x_test)
            else:
                # idx -> by learning
                idx = Learning().machine_learning(self.x_train, self.y_train, self.x_test, self._smallest_class)
            self._reveal(idx)
            # number of guesses/ number of nodes .. , for figure
        return self._time/self._num_nodes, self.y_train

    def _reveal(self, idx):
        self._time += 1
        self.bar.update() # display
        # if self._time % 100 == 0:
        #     print(str(self._time) + " nodes were explored, time:" + str(datetime.now().time()))
        if self.y_test[idx] == self._smallest_class:
            self._num_black_found += 1

        # add feature vec to train
        if self.x_train is None:
            self.x_train = self.x_test[idx, :]
            self.y_train = []
        else:
            self.x_train = np.vstack([self.x_train, self.x_test[idx, :]])
        # y train
        self.y_train.append(self.y_test.pop(idx))
        # delete feature vec from test
        self.x_test = np.delete(self.x_test, idx, 0)
